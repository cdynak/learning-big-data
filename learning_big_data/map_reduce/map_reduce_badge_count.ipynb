{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from uuid import uuid4\n",
    "\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    with open(f'./wordcount/{i}.txt', 'w') as f:\n",
    "        f.write(faker.paragraph(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt\t19.txt\t28.txt\t37.txt\t46.txt\t55.txt\t64.txt\t73.txt\t82.txt\t91.txt\r\n",
      "10.txt\t1.txt\t29.txt\t38.txt\t47.txt\t56.txt\t65.txt\t74.txt\t83.txt\t92.txt\r\n",
      "11.txt\t20.txt\t2.txt\t39.txt\t48.txt\t57.txt\t66.txt\t75.txt\t84.txt\t93.txt\r\n",
      "12.txt\t21.txt\t30.txt\t3.txt\t49.txt\t58.txt\t67.txt\t76.txt\t85.txt\t94.txt\r\n",
      "13.txt\t22.txt\t31.txt\t40.txt\t4.txt\t59.txt\t68.txt\t77.txt\t86.txt\t95.txt\r\n",
      "14.txt\t23.txt\t32.txt\t41.txt\t50.txt\t5.txt\t69.txt\t78.txt\t87.txt\t96.txt\r\n",
      "15.txt\t24.txt\t33.txt\t42.txt\t51.txt\t60.txt\t6.txt\t79.txt\t88.txt\t97.txt\r\n",
      "16.txt\t25.txt\t34.txt\t43.txt\t52.txt\t61.txt\t70.txt\t7.txt\t89.txt\t98.txt\r\n",
      "17.txt\t26.txt\t35.txt\t44.txt\t53.txt\t62.txt\t71.txt\t80.txt\t8.txt\t99.txt\r\n",
      "18.txt\t27.txt\t36.txt\t45.txt\t54.txt\t63.txt\t72.txt\t81.txt\t90.txt\t9.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repellendus voluptas molestias soluta quod aliquam. Delectus in cum architecto sint ullam. Ipsa dolor expedita ipsum reiciendis placeat. Magni enim voluptas voluptates doloremque eaque pariatur nam. Explicabo assumenda labore corrupti maiores explicabo. Ut inventore praesentium eum deleniti. Quo ea \r\n"
     ]
    }
   ],
   "source": [
    "!cut -c-300 ./wordcount/12.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Map Reduce Job runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def write(self, key, value):\n",
    "        self.events.append([key, value])\n",
    "        \n",
    "    def collect_to_dict(self):\n",
    "        collected = {}\n",
    "        for key, value in self.events:            \n",
    "            collected.setdefault(key, [])\n",
    "            collected[key].append(value)\n",
    "        \n",
    "        return collected\n",
    "    \n",
    "    \n",
    "class Job:\n",
    "    \n",
    "    def __init__(self, input_path, mapper, reducer):\n",
    "        self.input_path = input_path\n",
    "        self.mapper = mapper\n",
    "        self.reducer = reducer\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        # -- key\n",
    "        mapper_context = Context()        \n",
    "        for filepath in glob(self.input_path):\n",
    "            with open(filepath, 'r') as f:\n",
    "                self.mapper(\n",
    "                    key=filepath, \n",
    "                    value=f.read(), \n",
    "                    context=mapper_context)\n",
    "        \n",
    "        # -- reduce\n",
    "        reducer_context = Context()        \n",
    "        for key, values in mapper_context.collect_to_dict().items():\n",
    "            self.reducer(key, values, reducer_context)\n",
    "            \n",
    "        # -- write output\n",
    "        output_path = f'./outputs/{uuid4()}.txt'\n",
    "        with open(output_path, 'w') as f:\n",
    "            output = '\\n'.join([\n",
    "                '{0},{1}'.format(*event) \n",
    "                for event in reducer_context.events\n",
    "            ])            \n",
    "            f.write(output)\n",
    "            \n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Mapper and Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(key, value, context):\n",
    "    words = value.replace('.', '').lower().split()\n",
    "    \n",
    "    for word in words:\n",
    "        context.write(word, 1)\n",
    "\n",
    "        \n",
    "def reducer(key, values, context):\n",
    "    context.write(key, sum(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./outputs/7d45da75-82d4-4e3a-ad7c-ba521d9088f9.txt'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Job('./wordcount/*.txt', mapper, reducer).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culpa,330\r\n",
      "ipsum,326\r\n",
      "tenetur,350\r\n",
      "quas,363\r\n",
      "distinctio,355\r\n",
      "sit,346\r\n",
      "sed,348\r\n",
      "cumque,372\r\n",
      "repellendus,321\r\n",
      "deleniti,372\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ./outputs/7d45da75-82d4-4e3a-ad7c-ba521d9088f9.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations --> Homework\n",
    "\n",
    "1. Instead of connecting to local file system make all IO through S3 (or Azure Blob Storage) --> FIXME: prepare sample files!!!\n",
    "1. Try to paralelize the mapper and reducer (save context to files)\n",
    "1. Think about this job running on multiple machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
